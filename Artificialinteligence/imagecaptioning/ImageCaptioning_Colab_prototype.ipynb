{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Training Notebook\n",
    "This notebook downloads the Flickr8k dataset from Kaggle, prepares the data, trains the image captioning model, and reports training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Upload kaggle.json for Kaggle API authentication\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configure Kaggle API\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading flickr8k.zip to /content\n",
      " 99% 1.04G/1.05G [00:20<00:00, 56.3MB/s]\n",
      "100% 1.05G/1.05G [00:20<00:00, 53.3MB/s]\n",
      "\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of flickr8k.zip or\n",
      "        flickr8k.zip.zip, and cannot find flickr8k.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Download Flickr8k dataset from Kaggle\n",
    "!kaggle datasets download -d adityajn105/flickr8k\n",
    "!unzip -q flickr8k.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- If the unzip command fails, please manually upload the dataset or check the Kaggle API setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Install required packages\n",
    "!pip install torch torchvision pytorch-lightning nltk pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Import necessary modules\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pycocoevalcap.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "import random\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define Vocabulary class and build_vocab function\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        for token in ['<pad>', '<start>', '<end>', '<unk>']:\n",
    "            self.add_word(token)\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    def __call__(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx['<unk>'])\n",
    "    def __len__(self):\n",
    "        return self.idx\n",
    "def build_vocab(captions_file, freq_threshold=5):\n",
    "    with open(captions_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    counter = {}\n",
    "    for line in data:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            caption = parts[1]\n",
    "            tokens = nltk.word_tokenize(caption.lower())\n",
    "            for token in tokens:\n",
    "                counter[token] = counter.get(token, 0) + 1\n",
    "    vocab = Vocabulary()\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= freq_threshold:\n",
    "            vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Dataset class\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_file, vocab, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.captions_file = captions_file\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.img_captions = self._load_data()\n",
    "    def _load_data(self):\n",
    "        img_captions = []\n",
    "        with open(self.captions_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    img_info, caption = parts\n",
    "                    img_name = img_info.split('#')[0]\n",
    "                    img_captions.append((img_name, caption))\n",
    "        return img_captions\n",
    "    def __len__(self):\n",
    "        return len(self.img_captions)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.img_captions[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tokens = ['<start>'] + nltk.word_tokenize(caption.lower()) + ['<end>']\n",
    "        indices = [self.vocab(token) for token in tokens]\n",
    "        return image, torch.tensor(indices)\n",
    "    @staticmethod\n",
    "    def custom_collate_fn(batch):\n",
    "        batch = [item for item in batch if item[0] is not None]\n",
    "        images, captions = zip(*batch)\n",
    "        images = torch.stack(images, 0)\n",
    "        max_len = max(len(cap) for cap in captions)\n",
    "        padded_captions = torch.zeros((len(captions), max_len), dtype=torch.long)\n",
    "        for i, cap in enumerate(captions):\n",
    "            padded_captions[i, :len(cap)] = cap\n",
    "        return images, padded_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Define Model\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet50(weights=\"IMAGENET1K_V1\")\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images).squeeze()\n",
    "        return self.bn(self.linear(features))\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "class ImageCaptioningModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embed_size=256, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        outputs = self(images, captions)\n",
    "        loss = self.loss_fn(outputs.view(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        outputs = self(images, captions)\n",
    "        loss = self.loss_fn(outputs.view(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Define training parameters and transforms\n",
    "batch_size = 32\n",
    "max_epochs = 10\n",
    "learning_rate = 0.001\n",
    "image_dir = \"data/Flicker8k_Dataset\"\n",
    "captions_file = \"data/Flickr8k_text/Flickr8k.token.txt\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Build vocabulary, dataset, dataloader, and model\n",
    "vocab = build_vocab(captions_file)\n",
    "dataset = ImageCaptionDataset(image_dir, captions_file, vocab, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=ImageCaptionDataset.custom_collate_fn)\n",
    "model = ImageCaptioningModel(vocab_size=len(vocab), embed_size=256, hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Initialize PyTorch Lightning Trainer and train\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=max_epochs, precision=16)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: After training, you can save the model checkpoint\n",
    "trainer.save_checkpoint(\"image_captioning_model.ckpt\")\n",
    "\n",
    "# Step 13: To generate captions, load the model and run inference (not shown here but can be added)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 }
}

