{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34f1j6v_cf4m"
      },
      "source": [
        "# Handwritten Text Generation with PyTorch\n",
        "\n",
        "This notebook demonstrates generating handwritten-like text using a stroke-level recurrent neural network (RNN) implemented in PyTorch.\n",
        "\n",
        "We use the DeepWriting dataset (reference: [DeepWriting Dataset on PapersWithCode](https://paperswithcode.com/dataset/deepwriting)) which contains stroke-level handwriting data.\n",
        "\n",
        "The notebook covers dataset loading, model definition, training, inference, and saving the model."
      ],
      "id": "34f1j6v_cf4m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FJrKajIcf4o"
      },
      "source": [
        "## Setup\n",
        "Install necessary libraries if not already installed."
      ],
      "id": "0FJrKajIcf4o"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxaGl1dkcf4p",
        "outputId": "32eeade9-60af-4df9-eb88-a31d459422c6"
      },
      "source": [
        "!pip install torch numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "id": "PxaGl1dkcf4p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download DeepWriting dataset (already done in previous execution)\n",
        "# !mkdir data_set # Directory already exists\n",
        "# !wget https://files.ait.ethz.ch/projects/deepwriting/deepwriting_dataset.zip -P data_set/ # File already downloaded\n",
        "# !unzip data_set/deepwriting_dataset.zip -d data_set/ # Files already unzipped\n",
        "\n",
        "# Step 2: Load data from .npz files\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "training_data_path = \"data_set/deepwriting_training.npz\"\n",
        "validation_data_path = \"data_set/deepwriting_validation.npz\"\n",
        "\n",
        "all_strokes = None\n",
        "\n",
        "if os.path.exists(training_data_path):\n",
        "    training_data = np.load(training_data_path, allow_pickle=True)\n",
        "    if 'strokes' in training_data:\n",
        "        all_strokes = training_data['strokes']\n",
        "        print(f\"Loaded training data from {training_data_path}\")\n",
        "    else:\n",
        "        for key in training_data.keys():\n",
        "            if isinstance(training_data[key], np.ndarray):\n",
        "                all_strokes = training_data[key]\n",
        "                print(f\"Assuming key '{key}' contains the stroke data from {training_data_path}\")\n",
        "                break\n",
        "\n",
        "if os.path.exists(validation_data_path) and all_strokes is not None:\n",
        "    validation_data = np.load(validation_data_path, allow_pickle=True)\n",
        "    if 'strokes' in validation_data:\n",
        "        all_strokes = np.concatenate((all_strokes, validation_data['strokes']), axis=0)\n",
        "        print(f\"Loaded and concatenated validation data from {validation_data_path}\")\n",
        "    else:\n",
        "        for key in validation_data.keys():\n",
        "            if isinstance(validation_data[key], np.ndarray):\n",
        "                all_strokes = np.concatenate((all_strokes, validation_data[key]), axis=0)\n",
        "                print(f\"Assuming key '{key}' contains the stroke data from {validation_data_path} and concatenated.\")\n",
        "                break\n",
        "elif os.path.exists(validation_data_path) and all_strokes is None:\n",
        "     validation_data = np.load(validation_data_path, allow_pickle=True)\n",
        "     if 'strokes' in validation_data:\n",
        "         all_strokes = validation_data['strokes']\n",
        "         print(f\"Loaded validation data from {validation_data_path}\")\n",
        "     else:\n",
        "         for key in validation_data.keys():\n",
        "             if isinstance(validation_data[key], np.ndarray):\n",
        "                 all_strokes = validation_data[key]\n",
        "                 print(f\"Assuming key '{key}' contains the stroke data from {validation_data_path}\")\n",
        "                 break\n",
        "\n",
        "\n",
        "# Step 3: Save to .npy\n",
        "if all_strokes is not None:\n",
        "    np.save(\"/content/strokes.npy\", all_strokes, allow_pickle=True)\n",
        "    print(f\"✅ Saved {len(all_strokes)} stroke sequences to strokes.npy\")\n",
        "else:\n",
        "    print(\"❌ No stroke data was loaded from the .npz files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6_Walt4cj4n",
        "outputId": "a77043cb-d26e-4ecc-8a70-f80590cdce23"
      },
      "id": "l6_Walt4cj4n",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded training data from data_set/deepwriting_training.npz\n",
            "Loaded and concatenated validation data from data_set/deepwriting_validation.npz\n",
            "✅ Saved 35282 stroke sequences to strokes.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZapzBSBcf4q"
      },
      "source": [
        "## Dataset Loading\n",
        "\n",
        "Download or place the stroke-level dataset file `strokes.npy` in the working directory.\n",
        "\n",
        "You can download the dataset from [DeepWriting Dataset](https://paperswithcode.com/dataset/deepwriting).\n",
        "\n",
        "For this notebook, we assume the file `strokes.npy` is available locally."
      ],
      "id": "DZapzBSBcf4q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-s6E0svcf4r",
        "outputId": "6d092b8c-500f-422e-d4f1-c85a63c0eb7c"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "data_path = 'strokes.npy'\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"Dataset file {data_path} not found. Please download it from the DeepWriting dataset and place it in the working directory.\")\n",
        "else:\n",
        "    print(f\"Dataset file {data_path} found.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset file strokes.npy found.\n"
          ]
        }
      ],
      "id": "D-s6E0svcf4r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWwqAt7tcf4r"
      },
      "source": [
        "## Dataset Class\n",
        "Define a PyTorch Dataset class to load stroke sequences."
      ],
      "id": "KWwqAt7tcf4r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaDPi_0Vcf4r"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class StrokeDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        x = torch.tensor(seq[:-1], dtype=torch.float32)   # input (T-1, 3)\n",
        "        y = torch.tensor(seq[1:], dtype=torch.float32)    # target (T-1, 3)\n",
        "        return x, y"
      ],
      "execution_count": 10,
      "outputs": [],
      "id": "kaDPi_0Vcf4r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32orkf3Scf4s"
      },
      "source": [
        "## Model Definition\n",
        "Define the RNN model for handwriting generation."
      ],
      "id": "32orkf3Scf4s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxGbnjk_cf4s"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HandwritingRNN(nn.Module):\n",
        "    def __init__(self, input_size=3, hidden_size=256, num_layers=2, output_size=3, dropout_prob=0.2):\n",
        "        super(HandwritingRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            batch_first=True, dropout=dropout_prob\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Xavier initialization for LSTM weights\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "        nn.init.constant_(self.fc.bias, 0.0)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        out, hidden = self.lstm(x, hidden)            # LSTM output\n",
        "        out = self.layer_norm(out)                    # Layer Normalization\n",
        "        out = self.dropout(out)                       # Dropout\n",
        "        out = self.fc(out)                            # Final output layer\n",
        "        return out, hidden\n"
      ],
      "execution_count": 16,
      "outputs": [],
      "id": "LxGbnjk_cf4s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O38F1eqTcf4s"
      },
      "source": [
        "## Training Loop\n",
        "Define the training loop function."
      ],
      "id": "O38F1eqTcf4s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtJqTsW0cf4t"
      },
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, device, epochs=10, scheduler=None):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred, _ = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        # Step the scheduler at the end of each epoch\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")"
      ],
      "execution_count": 20,
      "outputs": [],
      "id": "BtJqTsW0cf4t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8AnA71Jcf4t"
      },
      "source": [
        "## Inference (Sampling)\n",
        "Define a function to generate handwriting sequences from a seed sequence."
      ],
      "id": "k8AnA71Jcf4t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZzOrsgZcf4t"
      },
      "source": [
        "def generate_sequence(model, seed_seq, length=300, device='cpu'):\n",
        "    model.eval()\n",
        "    generated = []\n",
        "    input_seq = torch.tensor(seed_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    hidden = None\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            out, hidden = model(input_seq, hidden)\n",
        "            next_point = out[:, -1, :].cpu().numpy()\n",
        "            generated.append(next_point)\n",
        "            input_seq = out[:, -1:, :]\n",
        "    return np.array(generated)"
      ],
      "execution_count": 18,
      "outputs": [],
      "id": "0ZzOrsgZcf4t"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFQx6Y6Zcf4t"
      },
      "source": [
        "## Main Training and Saving\n",
        "Set parameters, load dataset, create model, train, and save the model."
      ],
      "id": "NFQx6Y6Zcf4t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOeMowd5cf4t",
        "outputId": "ab0e3883-fee8-408b-b179-59ba2b878d68"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import numpy as np\n",
        "import torch.nn as nn # Import nn for criterion\n",
        "import torch # Import torch for scheduler\n",
        "\n",
        "# ---- Collate Function ----\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Pads sequences to the longest sequence in the batch.\"\"\"\n",
        "    sequences, targets = zip(*batch)\n",
        "    sequences_padded = rnn_utils.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    targets_padded = rnn_utils.pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "    return sequences_padded, targets_padded\n",
        "\n",
        "# ---- Training Parameters ----\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 64\n",
        "hidden_size = 256\n",
        "epochs = 50 # Increased epochs\n",
        "learning_rate = 0.001\n",
        "model_save_path = 'handwriting_rnn.pth'\n",
        "data_path = 'strokes.npy' # Assuming strokes.npy is in the current directory\n",
        "\n",
        "# Load dataset\n",
        "# Using the StrokeDataset class defined in a previous cell\n",
        "dataset = StrokeDataset(data_path)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
        "                        drop_last=True, collate_fn=collate_fn)\n",
        "\n",
        "# Create model\n",
        "# Using the HandwritingRNN class defined in a previous cell\n",
        "model = HandwritingRNN(hidden_size=hidden_size).to(device)\n",
        "\n",
        "# Optimizer and Criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss() # Using nn.MSELoss as in the original cell\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "\n",
        "# Train model\n",
        "# Using the train_model function defined in a previous cell\n",
        "train_model(model, dataloader, criterion, optimizer, device, epochs=epochs, scheduler=scheduler)\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"✅ Model saved at {model_save_path}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.4392\n",
            "Epoch 2/50, Loss: 0.2643\n",
            "Epoch 3/50, Loss: 0.2499\n",
            "Epoch 4/50, Loss: 0.2429\n",
            "Epoch 5/50, Loss: 0.2365\n",
            "Epoch 6/50, Loss: 0.2313\n",
            "Epoch 7/50, Loss: 0.2264\n",
            "Epoch 8/50, Loss: 0.2227\n",
            "Epoch 9/50, Loss: 0.2193\n",
            "Epoch 10/50, Loss: 0.2178\n",
            "Epoch 11/50, Loss: 0.2115\n",
            "Epoch 12/50, Loss: 0.2102\n",
            "Epoch 13/50, Loss: 0.2083\n",
            "Epoch 14/50, Loss: 0.2072\n",
            "Epoch 15/50, Loss: 0.2059\n",
            "Epoch 16/50, Loss: 0.2041\n",
            "Epoch 17/50, Loss: 0.2031\n",
            "Epoch 18/50, Loss: 0.2018\n",
            "Epoch 19/50, Loss: 0.2001\n",
            "Epoch 20/50, Loss: 0.1993\n",
            "Epoch 21/50, Loss: 0.1945\n",
            "Epoch 22/50, Loss: 0.1931\n",
            "Epoch 23/50, Loss: 0.1929\n",
            "Epoch 24/50, Loss: 0.1921\n",
            "Epoch 25/50, Loss: 0.1910\n",
            "Epoch 26/50, Loss: 0.1891\n",
            "Epoch 27/50, Loss: 0.1888\n",
            "Epoch 28/50, Loss: 0.1886\n",
            "Epoch 29/50, Loss: 0.1881\n",
            "Epoch 30/50, Loss: 0.1864\n",
            "Epoch 31/50, Loss: 0.1839\n",
            "Epoch 32/50, Loss: 0.1826\n",
            "Epoch 33/50, Loss: 0.1822\n",
            "Epoch 34/50, Loss: 0.1812\n",
            "Epoch 35/50, Loss: 0.1805\n",
            "Epoch 36/50, Loss: 0.1803\n",
            "Epoch 37/50, Loss: 0.1800\n",
            "Epoch 38/50, Loss: 0.1794\n",
            "Epoch 39/50, Loss: 0.1786\n",
            "Epoch 40/50, Loss: 0.1779\n",
            "Epoch 41/50, Loss: 0.1758\n",
            "Epoch 42/50, Loss: 0.1750\n",
            "Epoch 43/50, Loss: 0.1748\n",
            "Epoch 44/50, Loss: 0.1744\n",
            "Epoch 45/50, Loss: 0.1740\n",
            "Epoch 46/50, Loss: 0.1735\n",
            "Epoch 47/50, Loss: 0.1737\n",
            "Epoch 48/50, Loss: 0.1733\n",
            "Epoch 49/50, Loss: 0.1723\n",
            "Epoch 50/50, Loss: 0.1726\n",
            "✅ Model saved at handwriting_rnn.pth\n"
          ]
        }
      ],
      "id": "lOeMowd5cf4t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "437de41e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import numpy as np\n",
        "\n",
        "# Assuming StrokeDataset and HandwritingRNN classes are defined in previous cells\n",
        "# Assuming collate_fn is defined in a previous cell\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output, _ = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            total_loss += loss.item() * x.size(0)  # Weighted by batch size\n",
        "            total_samples += x.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    return avg_loss"
      ],
      "id": "437de41e",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78a9e184",
        "outputId": "b744a54b-2a0b-4041-c484-5c1d1b57d982"
      },
      "source": [
        "# ---- Evaluation Setup ----\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_save_path = 'handwriting_rnn.pth'\n",
        "# Assuming validation data is available or using the full dataset for evaluation\n",
        "# For demonstration, let's use the full dataset loaded into strokes.npy\n",
        "data_path = 'strokes.npy'\n",
        "\n",
        "# Load dataset\n",
        "dataset = StrokeDataset(data_path)\n",
        "# Using the same batch size and collate_fn as training for consistency\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, # No need to shuffle for eval\n",
        "                        drop_last=False, collate_fn=collate_fn) # Keep all samples for eval\n",
        "\n",
        "# Create model instance and load trained weights\n",
        "hidden_size = 256 # Must match the hidden size used for training\n",
        "model = HandwritingRNN(hidden_size=hidden_size).to(device)\n",
        "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "\n",
        "# Define Criterion\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Evaluate the model\n",
        "avg_eval_loss = evaluate_model(model, dataloader, criterion, device)\n",
        "print(f\"Evaluation Loss: {avg_eval_loss:.4f}\")"
      ],
      "id": "78a9e184",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 0.1658\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py",
      "nbformat": 4,
      "nbformat_minor": 5
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}